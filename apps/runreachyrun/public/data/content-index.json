{"documents": [{"id": "journal-huggingface-publish", "type": "journal", "title": "Publishing to HuggingFace: A Different Paradigm", "content": "Getting Focus Guardian into the HuggingFace/Pollen Robotics ecosystem required more than just uploading code. The entire architecture had to change.\n\n## Two Different Paradigms\n\nWhat we built (standalone):\n[code block]\n\nWhat Pollen expects (dashboard plugin):\n[code block]\n\nThe key insight: apps in the Reachy Mini ecosystem are **plugins**, not standalone applications.\n\n## The Refactor\n\nCreated new package structure with pyproject.toml, moved to ReachyMiniApp inheritance, added stop_event handling for clean shutdown.\n\n## The Publishing Moment\n\n[code block]\n\nResult: https://huggingface.co/spaces/RyeCatcher/focus-guardian\n\n## Plot Twist: It's a Static Site\n\nFirst deploy failed with `ModuleNotFoundError: No module named 'reachy_mini'`. Of course \u2014 the SDK runs locally where the robot is, not on HF servers!\n\nFixed by changing `sdk: gradio` to `sdk: static`. The Space is a *distribution point* (landing page + pip install source), not a running app.\n\n## Lesson Learned\n\nHuggingFace Spaces for Reachy Mini apps serve two purposes: (1) discovery/landing page, (2) pip-installable package. The actual app runs locally.", "metadata": {"date": "2025-12-21", "tags": ["apps", "huggingface", "ecosystem", "architecture"], "slug": "huggingface-publish", "url": "/journal/huggingface-publish"}}, {"id": "journal-first-boot", "type": "journal", "title": "First Physical Robot Boot (with Claude Code)", "content": "Built my Reachy Mini Lite overnight. Woke up, plugged it in via USB, and asked Claude Code for help getting it running. What followed was a real-time debugging session that showcases how AI-assisted development actually works.\n\n## The Problem: Stuck in Simulation Mode\n\nI had been running the simulator the day before. When I connected the physical robot, the daemon was still configured for simulation mode (`--sim` flag). The robot was physically connected but the software wasn't talking to it.\n\n**What I said:** \"I built my Reachy light. Can you help me get it running?\"\n\n**What Claude did:**\n\n1. Immediately checked USB detection: `ls /dev/tty.usb* /dev/cu.usb*`\n   Result: `/dev/cu.usbmodem5AF71342721` \u2014 the robot was detected!\n\n2. Checked daemon status: `ps aux | grep reachy`\n   Found the daemon running with `--sim --headless` flags.\n\n3. Diagnosed: Need to switch from sim to hardware mode.\n\n## First Movement: Proof of Life\n\n[code block]\n\nThe moment those antennas wiggled for the first time. Months of anticipation, reduced to a half-second of servo movement. It worked.\n\n## The Claude Code Advantage\n\n1. **Parallel investigation:** USB detection, daemon status, and doc search simultaneously\n2. **Code reading:** Actually read the conversation app source to understand API key validation\n3. **Security awareness:** Stored API key in Keychain, not plain text\n4. **Documentation:** Created this entry while working\n\nThis wasn't just \"robot works\" \u2014 it was a demonstration of AI-assisted hardware debugging.", "metadata": {"date": "2025-12-20", "tags": ["hardware", "debugging", "claude-code", "breakthrough"], "slug": "first-boot", "url": "/journal/first-boot"}}, {"id": "journal-first-day", "type": "journal", "title": "Day One: Setting Up the Documentation Site", "content": "Today marks the official start of documenting this build publicly. I've been working with the Reachy Mini Lite for about a week now, but everything before today was scattered notes and half-finished experiments.\n\nThe site itself is part of the story. I'm using Claude Code to help build it, which means the tool I'm using to document the robot project is also being documented. It's recursive in a way that feels appropriate for an AI project.\n\n## Technical Decisions\n\n- **Next.js 14 with App Router**: Server components where possible, client components for interactivity\n- **Tailwind CSS**: No component libraries \u2014 building everything custom\n- **Framer Motion**: For animations that feel intentional, not decorative\n- **Dark mode only**: This is a dev journal, not a marketing site\n\n## What's Working\n\nThe timeline component came together faster than expected. Claude Code helped iterate on the design \u2014 I described what I wanted, it generated options, I pushed back on the generic parts, and we landed somewhere interesting.\n\n## What's Next\n\nNeed to add the journal section (you're reading the first entry), GitHub integration for live commit data, and eventually embed some HuggingFace Spaces.", "metadata": {"date": "2025-12-21", "tags": ["meta", "launch", "claude-code", "next.js"], "slug": "first-day", "url": "/journal/first-day"}}, {"id": "journal-first-movement", "type": "journal", "title": "First Successful Coordinated Movement", "content": "Three hours. That's how long it took to understand why my head movements were going the wrong direction.\n\n## The Problem\n\nI kept sending commands like `goto_target(head=create_head_pose(z=20))` expecting the head to tilt forward. Instead, it was tilting... somewhere else. The documentation mentioned \"Z is forward/backward\" but forward relative to what?\n\n## The Debugging Session\n\nClaude Code was actually helpful here. I described the behavior I was seeing:\n\n> \"When I set z=20, the head tilts to the right instead of forward. When I set roll=20, it tilts forward.\"\n\nClaude's response pointed me to the coordinate frame origin. The SDK uses a body-centric coordinate system where:\n- **Z** is actually the vertical axis (up/down from the robot's perspective)\n- **Roll** controls the forward/backward tilt\n- **Yaw** controls left/right rotation\n\n## The Fix\n\n[code block]\n\n## The Moment\n\nWhen it finally worked \u2014 when Reachy turned its head to look at me and wiggled its antennas \u2014 I literally laughed out loud. There's something about a robot responding to your commands correctly for the first time that never gets old.\n\n## Lesson Learned\n\nAlways verify coordinate systems empirically. Documentation can be ambiguous. Send small test commands and observe before building complex sequences.", "metadata": {"date": "2025-12-16", "tags": ["software", "sdk", "debugging", "coordinates"], "slug": "first-movement", "url": "/journal/first-movement"}}, {"id": "journal-camera-debugging", "type": "journal", "title": "Camera Integration: A Dead End (For Now)", "content": "Today was supposed to be the day I got head pose detection working. It wasn't.\n\n## The Goal\n\nI wanted Reachy to track faces and maintain eye contact during interactions. The SDK has camera support, so this seemed straightforward.\n\n## What Happened\n\nThe camera works fine when running the daemon with a GUI window. But I'm running in headless mode (no physical display attached), and that's where things break.\n\n[code block]\n\n## Debugging Attempts\n\n1. **Different camera initialization flags** \u2014 No effect\n2. **Forcing OpenCV backend** \u2014 Same timeout\n3. **Checking if MuJoCo sim provides camera** \u2014 It does, but only with the 3D window open\n4. **SDK source code dive** \u2014 The camera thread expects a running render loop\n\n## The Reality\n\nHeadless mode and camera input are architecturally at odds in the current SDK. The camera depends on the render pipeline, which doesn't run without a window.\n\n## Options\n\n1. Run with a virtual framebuffer (Xvfb) \u2014 hacky but might work\n2. Use external camera + separate face detection pipeline \u2014 more work, but cleaner\n3. Wait for SDK update \u2014 there's a GitHub issue open about this\n4. Work around it \u2014 build features that don't need camera for now\n\n## Decision\n\nParking this. I'll focus on apps that use the robot's movements and expressions without real-time vision. The Focus Guardian app can work with keyboard/manual input initially.\n\n## Mood\n\nFrustrated but realistic. Not every session ends with a win. Documenting the dead ends is part of the process.", "metadata": {"date": "2025-12-17", "tags": ["software", "camera", "blocker", "debugging"], "slug": "camera-debugging", "url": "/journal/camera-debugging"}}, {"id": "journal-focus-guardian-prd", "type": "journal", "title": "Focus Guardian: The Concept", "content": "Had an idea while procrastinating (ironic): what if Reachy could be a body double?\n\n## The Concept\n\n\"Body doubling\" is a productivity technique where having another person present \u2014 even silently \u2014 helps you focus. It's especially useful for ADHD brains. The other person doesn't have to do anything; their presence creates gentle accountability.\n\nWhat if a robot could do this?\n\n## Focus Guardian Features (v1)\n\n- **Presence mode**: Reachy sits attentively, occasionally shifting position to feel \"alive\"\n- **Focus timer**: Pomodoro-style work sessions with animated transitions\n- **Gentle check-ins**: Subtle movements or sounds if you've been idle too long\n- **Celebration**: Antenna wiggles and happy expressions when you complete a session\n\n## Why This Could Work\n\n1. It's low-stakes \u2014 no judgment, no human awkwardness\n2. The robot is inherently engaging \u2014 you want to \"perform\" for it\n3. It gamifies focus without being annoying about it\n4. It works without camera (see yesterday's frustration)\n\n## Technical Approach\n\n- Gradio UI for the timer and controls\n- Keyboard input for \"I'm working\" / \"I'm distracted\" signals (for now)\n- Pre-built animation sequences for different states\n- Optional screen time tracking via system APIs\n\n## Open Questions\n\n- How often should it move to feel present without being distracting?\n- What's the right balance of encouragement vs. leaving you alone?\n- Should it track breaks, or just work sessions?\n\n## Next Steps\n\nWriting a proper PRD and starting the Gradio scaffold.", "metadata": {"date": "2025-12-18", "tags": ["apps", "focus-guardian", "concept", "productivity"], "slug": "focus-guardian-prd", "url": "/journal/focus-guardian-prd"}}, {"id": "journal-simulation-setup", "type": "journal", "title": "MuJoCo Simulation: Now We're Cooking", "content": "Major infrastructure win today. The MuJoCo simulation is working, which means I can:\n\n1. Develop when the robot isn't connected\n2. Test potentially dangerous movements safely\n3. Iterate faster (no physical constraints)\n4. Record demos without the physical hardware\n\n## Setup\n\nThe SDK ships with MuJoCo 3.3.0 support. Running in simulation mode:\n\n[code block]\n\nThe `--headless` flag is key for running from scripts/CI. Without it, MuJoCo tries to open a 3D window.\n\n## What Works\n\n- All movement commands\n- Antenna control\n- Head poses\n- Position feedback\n- The REST API at localhost:8000\n\n## What Doesn't\n\n- Camera (see future debugging session)\n- Audio (no physical speakers to simulate)\n- Some edge cases in collision detection\n\n## LaunchAgent Setup\n\nI set up a LaunchAgent so the daemon starts automatically on login:\n\n[code block]\n\nNow I can just write code and trust the daemon is running. If it crashes, launchd restarts it.\n\n## Feeling\n\nProductive. Having the simulation layer means I can move faster on software without being blocked by hardware logistics.", "metadata": {"date": "2025-12-14", "tags": ["software", "simulation", "mujoco", "infrastructure"], "slug": "simulation-setup", "url": "/journal/simulation-setup"}}, {"id": "journal-dj-reactor-start", "type": "journal", "title": "DJ Reactor: Making Reachy Dance", "content": "New app idea: DJ Reactor. Reachy reacts to music in real-time.\n\n## The Vision\n\nPlay any song, and Reachy:\n- Bobs its head to the beat\n- Wiggles antennas on drops\n- Changes expressions based on energy level\n- Maybe tracks specific instruments\n\n## Technical Approach\n\nUsing `librosa` for audio analysis:\n\n[code block]\n\nThen mapping these features to robot movements:\n- Beat \u2192 head bob (quick up/down on Z)\n- Energy \u2192 antenna spread (more energy = wider)\n- Onset detection \u2192 eye reactions\n\n## Challenges\n\n1. **Latency**: Audio analysis needs to be fast enough for real-time response\n2. **Smoothing**: Raw beat detection is jittery; need to smooth movements\n3. **Not looking stupid**: Easy to make the robot look like it's having a seizure\n\n## First Prototype\n\nGot basic beat detection working. Reachy bobs on every detected beat. It's... okay. Needs work on timing and amplitude.\n\n## What's Next\n\n- Add energy-based modulation\n- Implement anticipation (move slightly before the beat, not after)\n- Build a simple Gradio UI for track selection\n\nThis one's going to be fun.", "metadata": {"date": "2025-12-20", "tags": ["apps", "dj-reactor", "audio", "librosa"], "slug": "dj-reactor-start", "url": "/journal/dj-reactor-start"}}, {"id": "blog-first-boot-with-claude", "type": "blog", "title": "First Boot: Debugging a Robot with an AI Pair Programmer", "content": "I built my Reachy Mini Lite overnight. Woke up at 6am, plugged it in via USB, and asked Claude Code for help getting it running. Forty-five minutes later, the robot was talking back to me.\n\nThis is the story of that debugging session. Not the polished \"and then it worked\" version. The actual sequence of problems, wrong turns, and the moments where having an AI read source code in real-time made the difference.\n\nI've [written about AI coding assistants before](https://rundatarun.io/p/a-deep-dive-into-ai-coding-assistants) and [compared Claude Code to other tools](https://rundatarun.io/p/the-battle-for-ai-powered-development). This is the first time I've used one for hardware debugging. The experience was different from software development in ways I didn't expect.\n\n## The Setup\n\nI'd been running the Reachy Mini in simulation mode for a week, building and testing apps. The simulator uses MuJoCo for physics, and the code works identically on hardware. In theory.\n\nThe physical robot arrived. I assembled it, connected via USB, and got nothing. Well, not nothing. The robot was detected. It just wasn't responding to commands.\n\n## Stuck in Simulation Mode\n\nThe daemon (the background service that manages robot communication) was still configured for simulation. I'd been running `--sim --headless` flags for a week. The robot was physically there, but the software wasn't talking to it.\n\nMy prompt was simple: \"I built my Reachy light. Can you help me get it running?\"\n\nClaude checked USB detection first:\n\n[code block]\n\nResult: `/dev/cu.usbmodem5AF71342721`. The robot was detected.\n\nThen daemon status:\n\n[code block]\n\nFound it running with `--sim --headless --fastapi-port 8000`. Simulation mode. The diagnosis was immediate: stop the sim daemon, start one for hardware.\n\n## Serial Port Locking\n\nClaude stopped the simulation daemon and tried to start the hardware daemon. Error: \"Device or resource busy.\"\n\nThe serial port was still locked by the previous process. Python's serial...", "metadata": {"date": "2025-12-20", "tags": ["hardware", "claude-code", "debugging", "robotics"], "slug": "first-boot-with-claude", "url": "/blog/first-boot-with-claude"}}, {"id": "timeline-huggingface-publish", "type": "timeline", "title": "First HuggingFace Space published", "content": "Refactored Focus Guardian to ReachyMiniApp format and published to HuggingFace Spaces. First step toward the official app store.\n\nhuggingface-publish\n\nQ: How do I get Focus Guardian into the Pollen Robotics app ecosystem?\n\nA: The ecosystem expects apps to inherit from ReachyMiniApp, not run standalone. Your app receives a pre-initialized robot and must respect stop_event for clean shutdown...", "metadata": {"date": "2025-12-21", "tags": ["apps", "huggingface", "ecosystem", "milestone"], "url": "/timeline#huggingface-publish"}}, {"id": "timeline-site-launch", "type": "timeline", "title": "runreachyrun.com goes live", "content": "Launched the documentation site for the Reachy Mini Lite build. Built with Next.js 14, Tailwind, and Claude Code.\n\nfirst-day\n\nQ: I want to brainstorm the design direction. Show me 2-3 distinct visual directions we could take.\n\nA: Let me give you three distinct visual directions. These aren't incremental variations \u2014 they're philosophically different approaches: Transmission (signal aesthetic), Exposed Circuitry (PCB traces), Dual Presence (visible AI layer)...", "metadata": {"date": "2025-12-21", "tags": ["meta", "launch", "claude-code"], "url": "/timeline#site-launch"}}, {"id": "timeline-first-physical-boot", "type": "timeline", "title": "First physical robot boot", "content": "45 minutes from 'help me get it running' to a talking robot. Claude Code debugged sim-vs-hardware mode, serial port locking, and API key storage in real-time.\n\nfirst-boot\n\nQ: I built my Reachy light. Can you help me get it running?\n\nA: Let me check USB detection first... Found /dev/cu.usbmodem5AF71342721. Now checking daemon status... It's running in --sim mode. Need to switch to hardware mode.", "metadata": {"date": "2025-12-20", "tags": ["hardware", "debugging", "claude-code", "breakthrough"], "url": "/timeline#first-physical-boot"}}, {"id": "timeline-dj-reactor-complete", "type": "timeline", "title": "DJ Reactor MVP complete", "content": "Built music visualizer with 7 genre presets. Real-time beat detection, FFT analysis, and synchronized robot movements. The robot dances!\n\ndj-reactor-start", "metadata": {"date": "2025-12-20", "tags": ["apps", "dj-reactor", "audio", "breakthrough"], "url": "/timeline#dj-reactor-complete"}}, {"id": "timeline-reachy-unboxing", "type": "timeline", "title": "Reachy Mini Lite arrives", "content": "Unboxed the Reachy Mini Lite. First impressions: surprisingly compact, excellent build quality, immediate desire to make it wave.", "metadata": {"date": "2025-12-15", "tags": ["hardware", "unboxing"], "url": "/timeline#reachy-unboxing"}}, {"id": "timeline-first-movement", "type": "timeline", "title": "First successful movement", "content": "Got Reachy to execute a coordinated head turn and antenna wiggle. Took 3 hours to figure out the coordinate system.\n\nfirst-movement\n\nQ: Help me understand the coordinate system for head movements\n\nA: The Reachy Mini uses a coordinate system where Z is forward/backward head tilt, roll is left/right head tilt...", "metadata": {"date": "2025-12-16", "tags": ["software", "sdk", "breakthrough"], "url": "/timeline#first-movement"}}, {"id": "timeline-camera-issues", "type": "timeline", "title": "Camera integration blocked", "content": "Attempted to integrate head pose detection. Camera timeout issues in headless mode. Need to investigate SDK camera handling.\n\ncamera-debugging", "metadata": {"date": "2025-12-17", "tags": ["software", "camera", "blocker"], "url": "/timeline#camera-issues"}}, {"id": "timeline-focus-guardian-concept", "type": "timeline", "title": "Focus Guardian app concept", "content": "Brainstormed a productivity app: Reachy as a body double that tracks focus and provides gentle accountability.\n\nfocus-guardian-prd", "metadata": {"date": "2025-12-18", "tags": ["apps", "focus-guardian", "concept"], "url": "/timeline#focus-guardian-concept"}}, {"id": "timeline-daemon-setup", "type": "timeline", "title": "Daemon auto-start configured", "content": "Set up LaunchAgent for the Reachy daemon. Now starts automatically on login and restarts if it crashes.", "metadata": {"date": "2025-12-19", "tags": ["software", "infrastructure", "daemon"], "url": "/timeline#daemon-setup"}}, {"id": "timeline-simulation-working", "type": "timeline", "title": "MuJoCo simulation running", "content": "Got the MuJoCo physics simulation working in headless mode. Can now develop without the physical robot connected.\n\nsimulation-setup", "metadata": {"date": "2025-12-14", "tags": ["software", "simulation", "mujoco"], "url": "/timeline#simulation-working"}}, {"id": "app-focus-guardian", "type": "app", "title": "Focus Guardian", "content": "A productivity body-double app that uses Reachy Mini as an accountability partner. The robot watches you work, notices when you get distracted, and provides gentle encouragement through expressions and movements. Based on the body-doubling technique used for ADHD focus.\n\nFeatures: Attention Tracking, Pomodoro Sessions, Expressive Feedback, Session Analytics\n\nHow it works: Start a Focus Session \u2192 Work While Watched \u2192 Get Gentle Nudges \u2192 Celebrate Completion\n\nTech stack: Python, Gradio, MediaPipe, OpenCV, Reachy SDK", "metadata": {"slug": "focus-guardian", "url": "/apps/focus-guardian"}}, {"id": "app-dj-reactor", "type": "app", "title": "DJ Reactor", "content": "An audio-reactive experience where Reachy Mini responds to music in real-time. The robot analyzes audio frequencies, detects beats, and translates sound into synchronized movements \u2014 head bobs, antenna waggles, and LED color changes.\n\nFeatures: Real-time Audio Analysis, Beat Detection, LED Visualization, Movement Library\n\nHow it works: Audio Input \u2192 Frequency Analysis \u2192 Beat Sync \u2192 Expressive Output\n\nTech stack: Python, Gradio, NumPy, librosa, Reachy SDK", "metadata": {"slug": "dj-reactor", "url": "/apps/dj-reactor"}}, {"id": "app-reachy-companion", "type": "app", "title": "Reachy Companion", "content": "A comprehensive control interface for your Reachy Mini. Browse and trigger expressions, run demo sequences, configure behaviors, monitor robot status, and customize your robot's personality \u2014 all from a clean Gradio interface.\n\nFeatures: Expression Browser, Demo Sequences, Behavior Configuration, Status Dashboard\n\nHow it works: Connect to Robot \u2192 Browse Expressions \u2192 Trigger & Customize \u2192 Save Presets\n\nTech stack: Python, Gradio, Reachy SDK, WebSocket", "metadata": {"slug": "reachy-companion", "url": "/apps/reachy-companion"}}, {"id": "claude-hardware-debugging", "type": "claude-session", "title": "Get physical Reachy Mini running from scratch", "content": "45 minutes from 'help me get it running' to a talking robot. Debugged sim-vs-hardware daemon modes, serial port locking, and API key storage in real-time. Claude diagnosed the sim-vs-hardware daemon issue, fixed serial port locking with pkill, tested antenna movement, and traced API key validation through source code.", "metadata": {"date": "2025-12-20", "url": "/claude#hardware-debugging"}}, {"id": "claude-huggingface-ecosystem", "type": "claude-session", "title": "Publish Focus Guardian to HuggingFace Spaces", "content": "Discovered the Pollen Robotics ecosystem expects dashboard plugins, not standalone apps. Refactored to ReachyMiniApp pattern and learned Spaces serve as distribution points. Apps receive pre-initialized robots and respect stop_event.", "metadata": {"date": "2025-12-21", "url": "/claude#huggingface-ecosystem"}}, {"id": "claude-dj-reactor-audio", "type": "claude-session", "title": "Build real-time audio-reactive robot movements", "content": "Created DJ Reactor with 7 genre presets. Claude designed the audio pipeline architecture with FFT analysis, beat detection, and movement mapping running in parallel threads. Real-time beat detection with ~50ms latency.", "metadata": {"date": "2025-12-20", "url": "/claude#dj-reactor-audio"}}, {"id": "claude-design-system", "type": "claude-session", "title": "Create unique visual identity for runreachyrun.com", "content": "Developed the signal-inspired design system with dark theme, cyan/amber accents, and animated components. The 'transmission active' motif emerged from discussing what makes robot communication feel distinct.", "metadata": {"date": "2025-12-21", "url": "/claude#design-system"}}, {"id": "claude-timeline-architecture", "type": "claude-session", "title": "Build interactive timeline as site centerpiece", "content": "Created expandable timeline nodes with filtering by type and tag, media grids, commit links, and Claude snippet display. Bidirectional linking to journal entries. Progressive disclosure prevents overwhelming users.", "metadata": {"date": "2025-12-21", "url": "/claude#timeline-architecture"}}, {"id": "claude-github-integration", "type": "claude-session", "title": "Show live GitHub activity with graceful fallbacks", "content": "Built API route with caching, rate limit awareness, and fallback data. Dashboard shows recent commits and repo stats, updating every 5 minutes. Always design for failure - fallback data means site never shows broken state.", "metadata": {"date": "2025-12-21", "url": "/claude#github-integration"}}, {"id": "claude-this-page", "type": "claude-session", "title": "Create dedicated page documenting Claude Code's role", "content": "The recursive nature of AI-assisted development - Claude building the page that documents Claude building the project. Meta documentation capturing prompts and learnings for future sessions.", "metadata": {"date": "2025-12-21", "url": "/claude#this-page"}}], "relatedMap": {"journal-huggingface-publish": ["timeline-huggingface-publish", "claude-huggingface-ecosystem", "journal-first-day"], "journal-first-boot": ["blog-first-boot-with-claude", "timeline-first-physical-boot", "claude-hardware-debugging"], "journal-first-day": ["timeline-site-launch", "claude-this-page", "journal-huggingface-publish"], "journal-first-movement": ["timeline-first-movement", "journal-first-boot", "journal-simulation-setup"], "journal-camera-debugging": ["timeline-camera-issues", "journal-simulation-setup", "journal-huggingface-publish"], "journal-focus-guardian-prd": ["app-focus-guardian", "timeline-focus-guardian-concept", "app-reachy-companion"], "journal-simulation-setup": ["timeline-simulation-working", "journal-first-boot", "blog-first-boot-with-claude"], "journal-dj-reactor-start": ["app-dj-reactor", "claude-dj-reactor-audio", "timeline-dj-reactor-complete"], "blog-first-boot-with-claude": ["journal-first-boot", "timeline-first-physical-boot", "claude-hardware-debugging"], "timeline-huggingface-publish": ["claude-huggingface-ecosystem", "journal-huggingface-publish", "timeline-focus-guardian-concept"], "timeline-site-launch": ["journal-first-day", "journal-dj-reactor-start", "app-reachy-companion"], "timeline-first-physical-boot": ["journal-first-boot", "blog-first-boot-with-claude", "claude-hardware-debugging"], "timeline-dj-reactor-complete": ["claude-dj-reactor-audio", "app-dj-reactor", "journal-dj-reactor-start"], "timeline-reachy-unboxing": ["timeline-site-launch", "app-reachy-companion", "claude-hardware-debugging"], "timeline-first-movement": ["journal-first-movement", "claude-hardware-debugging", "journal-dj-reactor-start"], "timeline-camera-issues": ["journal-camera-debugging", "journal-simulation-setup", "timeline-simulation-working"], "timeline-focus-guardian-concept": ["app-focus-guardian", "journal-focus-guardian-prd", "timeline-huggingface-publish"], "timeline-daemon-setup": ["claude-hardware-debugging", "app-reachy-companion", "timeline-first-physical-boot"], "timeline-simulation-working": ["journal-simulation-setup", "journal-first-boot", "journal-camera-debugging"], "app-focus-guardian": ["timeline-focus-guardian-concept", "journal-focus-guardian-prd", "app-reachy-companion"], "app-dj-reactor": ["journal-dj-reactor-start", "claude-dj-reactor-audio", "timeline-dj-reactor-complete"], "app-reachy-companion": ["app-dj-reactor", "app-focus-guardian", "journal-dj-reactor-start"], "claude-hardware-debugging": ["timeline-first-physical-boot", "journal-first-boot", "blog-first-boot-with-claude"], "claude-huggingface-ecosystem": ["timeline-huggingface-publish", "journal-huggingface-publish", "app-focus-guardian"], "claude-dj-reactor-audio": ["timeline-dj-reactor-complete", "app-dj-reactor", "journal-dj-reactor-start"], "claude-design-system": ["timeline-site-launch", "claude-dj-reactor-audio", "app-dj-reactor"], "claude-timeline-architecture": ["journal-first-day", "claude-this-page", "timeline-site-launch"], "claude-github-integration": ["journal-first-day", "journal-huggingface-publish", "claude-huggingface-ecosystem"], "claude-this-page": ["journal-first-day", "claude-timeline-architecture", "timeline-site-launch"]}, "generatedAt": "2025-12-22T07:49:09.510827"}